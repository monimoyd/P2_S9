{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T3D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9XD3Mal4yWR46PRZsy+jF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monimoyd/P2_S9/blob/master/T3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7SiSlovxzRI",
        "colab_type": "text"
      },
      "source": [
        "## Phase2 Assignment 9\n",
        "\n",
        "In this assignment, I have privided steps for T3D RL algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOXAIRDGyec3",
        "colab_type": "text"
      },
      "source": [
        "## install all the modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij-yPF6QyRur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "d0b05f08-1794-4aa8-ef32-ab741106b509"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov1PnHAI7XXr",
        "colab_type": "code",
        "outputId": "4c52a316-dec9-4d50-bc2c-63709a98e52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaNTewml7uyj",
        "colab_type": "code",
        "outputId": "231c4acd-b1d2-47be-f683-601b59910eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        }
      },
      "source": [
        "!pip install agents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting agents\n",
            "  Downloading https://files.pythonhosted.org/packages/51/ad/1f5ec523ab4fdafdb36c2a9a2e0b7554ae961a261091b15fdfd254c2f286/agents-1.4.0.tar.gz\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from agents) (2.2.0rc2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from agents) (0.17.1)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.6/dist-packages (from agents) (0.16.10)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (2.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.18.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (2.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (2.2.0rc0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (3.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (1.27.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->agents) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->agents) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->agents) (1.5.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from ruamel.yaml->agents) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (1.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (3.2.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (1.6.0.post2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (0.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->agents) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->agents) (3.1.0)\n",
            "Building wheels for collected packages: agents\n",
            "  Building wheel for agents (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for agents: filename=agents-1.4.0-cp36-none-any.whl size=62746 sha256=dd21b8203283ef3c001505d4f321c3cac20bcca4d6edb7440b78b5c1b10b8021\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/23/05/e3ed899566d6d9e52aa5c0e7c0f454f4e474b7d01cf5ad106b\n",
            "Successfully built agents\n",
            "Installing collected packages: agents\n",
            "Successfully installed agents-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0htPZ6073Xy",
        "colab_type": "code",
        "outputId": "e618ede4-786d-45dd-9f55-648fb98d8eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "!pip install ruamel.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.6/dist-packages (0.16.10)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from ruamel.yaml) (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk84hW48yje1",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZSkzxuh8B2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtAsVxd_yq42",
        "colab_type": "text"
      },
      "source": [
        "## Create Repplay Buffer Class\n",
        "\n",
        "- add method of class used for adding new transitions\n",
        "- sample method of class is used for sampling list of transitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKvL2h-K9Sy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition)\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, len(self.storage), batch_size)\n",
        "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "        for i in ind:\n",
        "            state, next_state, action,reward,done = self.storage[i]\n",
        "            batch_states.append(np.array(state, copy=False))\n",
        "            batch_next_states.append(np.array(next_state, copy=False))\n",
        "            batch_rewards.append(np.array(reward, copy=False))\n",
        "            batch_dones.append(np.array(done, copy=False))\n",
        "            batch_states.append(np.array(state, copy=False))\n",
        "        return np.array(batch_states), np.array(batch_next_states),  np.array(batch_actions), np.array(batch_rewards).reshape(-1,1), np.array(batch_dones).reshape(-1,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE7cznuBzHqi",
        "colab_type": "text"
      },
      "source": [
        "## Define Actor class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWWbz8Of-lN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dims, action_dims, max_action):\n",
        "        # max action is to clip in case we added too much noise\n",
        "        super(Actor, self).__init__()\n",
        "        self.layer_1 = nn.Linear(state_dims, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxd5bgVdzPS7",
        "colab_type": "text"
      },
      "source": [
        "## Define Critic Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IujXunSAVvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dims, action_dim):\n",
        "        self.layer_1 = nn.Linear(state_dims + action_dim, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, action_dim)\n",
        "        self.layer_4 = nn.Linear(state_dims + action_dim, 400)\n",
        "        self.layer_5 = nn.Linear(400, 300)\n",
        "        self.layer_6 = nn.Linear(300, action_dim)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "\n",
        "        x2 = F.relu(self.layer_4(xu))\n",
        "        x2 = F.relu(self.layer_5(x2))\n",
        "        x2 = self.layer_6(x2)\n",
        "\n",
        "        return x1, x2\n",
        "    \n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "        return x1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjvQe0N5zV8P",
        "colab_type": "text"
      },
      "source": [
        "## Define T3D class\n",
        "- T3D class creates an instance of Actor and Actor target\n",
        "- Create instance of Critic1 and Critic1 target, Critic2 and Critic2 target\n",
        "- taing method is used for training T3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pndOCflEDB4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class T3D(object):\n",
        "    def __init__(self, state_dims, action_dim, max_action):\n",
        "        self.actor = Actor(state_dims, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dims, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict)\n",
        "        # Initializing with model weights to keep then same\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "        self.critic = Critic(state_dims, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dims, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict)\n",
        "        self.critic_optimizer = torch.optim.Adam(seld.critic.parameterss())\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu.data.numpy().flatten()\n",
        "\n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau = 0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        for it in range(iterations):\n",
        "            # Step 4 We sample from a batch of transitions (s, s', a, r) from memory\n",
        "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "            state = torch.Tensor(batch_states).to(device)\n",
        "            next_state = torch.Tensor(batch_next_states).to(device)\n",
        "            action = torch.Tensor(batch_actions).to(device)\n",
        "            reward = torch.Tensor(batch_rewards).to(device)\n",
        "            done = torch.Tensor(batch_dones).to(device)\n",
        "            # Step5: From the next state s', the Actor target plays the next action a'\n",
        "            next_action = self.actor_target.forward(next_state)\n",
        "\n",
        "            # Step6: We add Gaussian noise to this next action a' and we clamp it in a \n",
        "            # range of values supported by the environment\n",
        "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Step7: The two Critic targets take each the couple (s', a') as input and \n",
        "            # return two Q values, Qt1(s', a') and Qt2(s', a') as outputs\n",
        "            target_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)\n",
        "\n",
        "            # Step8: We keep the minimum of these two Q values\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "            # Step 9: We get the final target of the two Critic model, whish is:\n",
        "            # Qt = r + gamma*min(Qt1, Qt2)\n",
        "            # target_Q = reward + (1-done)* discount * target_Q\n",
        "            # done 0 means episode not over, 1- episode over\n",
        "            # We can't run the above equation efficiently as some components are in Computational \n",
        "            # graphs and some are not. We need to make one minor modification\n",
        "            target_Q = reward + ((1-done)* discount *target_Q).detach()\n",
        "\n",
        "            # Step 10: the two critic models take each the couple (s,a),\n",
        "            # as input adn return two Q values\n",
        "            current_Q1, current_Q2 = self.critic.forward(state, action)\n",
        "\n",
        "            # Step 11: We compute the loss coming from two Critic models\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "            # Step 12: We backpropagate teh Critic loss and update the parameters of the two Critic\n",
        "            # models with a Adam optimizer\n",
        "            self.critic_optimizer.zero_grad() # initializing the gradients to zero\n",
        "            critic_loss.backward() # computing the gradients\n",
        "            self.crtic_optimizer.step() # performing the wegiht updates\n",
        "\n",
        "            # Step 13: Once every two iterations, we updte our Actor model by \n",
        "            # performing gradient ascent on the output of the first critic model\n",
        "            if it % policy_freq == 0:\n",
        "                # This is DPG part\n",
        "                actor_loss = -(self.critic.Q1(state, self.actor(state)).mean())\n",
        "                self.actor_optimizer.grad_zero()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "            # Step 14: Still once every two iterations, we update the weights of teh Actor target\n",
        "            # by Polyak averaging\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + ( 1 - tau) * target_param.data)\n",
        "            \n",
        "\n",
        "             #Step 15: Still once every tow iterations, we update the weights of the Critic target\n",
        "             # by Polyak averaging\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}